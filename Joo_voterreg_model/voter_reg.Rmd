---
title: "voter_reg_modeling"
output: html_document
---

# 1. Sources & Libraries 

```{r, quietly=TRUE}
source('config/init.R')
source('config/pgConnect.R')
source('config/voter_regfunctions.R') # all functions are here
source('config/data_source.R') # data sources
pg <- pgConnect()

packages.used <- c("lubridate", "ggplot2", "caret", 
                   "reshape2", "dplyr", 
                   "MLmetrics", "e1071", "tree", "rstanarm", 
                   "doMC", "xgboost") # update

#check packages that need to be installed
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

#install additional packages 
if (length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE)
}

library(lubridate)
library(reshape2)
library(caret)
library(ggplot2)
library(MLmetrics)
library(nnet)
library(doMC)
library(mice)
library(xgboost)
library(gbm)
library(randomForest)
library(zoo)
library(rlang)
library(dplyr)

options(mc.cores = parallel::detectCores())
```

# 2. Load datasets (Depending on when they were refreshed)

```{r}
data_list <- load_data(30)
list2env(data_list, .GlobalEnv)
```

# Data wrangling 

## Remove records of under 18 northstar_ids in all datasets 

```{r}
# only keep rows whose age < 18, in US, and members in relevant datasets. 
phoenix <- remove18(phoenix, users)
campaign <- remove18(campaign, users)
email <- remove18(email, users)
sms <- remove18(sms, users)
turbo <- remove18(turbo, users)
mel <- remove18(mel, users)
```

## Extracting predicted feature (voter registration status)

```{r}
# a part of this code will soon become obsolete as Jen is leading the sprint to combine for all columns 
reg_status <- predicted_feature(users, turbo, campaign, phoenix, email)
```


## Extracting demographic features 

```{r}
# gender feature 
gender <- gender_feature(users)

# age feature 
age <- age_feature(users)

# SES feature 
ses <- ses_feature(users)

# state and whether it turned blue/red in 2016 election 
state_politics <- dem_rep_feature(users)
```

## Extracting behavioral features 

```{r}
# returns variables of email clicked, converted, opened, unsubscribed & total number of email actions per northstar_id  
email_count <- email_feature(email)

# returns columns of SMS action by type & total number of sms actions per northstar_id 
sms_count <- sms_total_feature(sms)

# returns columns of browser size by northstar_id 
browser_size <- browser_size_feature(phoenix)

# returns number of total actions per northstar_id 
total_action <- total_actions_feature(phoenix, mel)

# how many reportbacks a user has done 
rb_status <- rb_feature(campaign, users)

# returns time of day activity level (day broken down by "morning", "afternoon", "evening", "night")
time_of_day <- timeday_feature(mel, users) # potentially recategorize using tree.bins or rpart or recat? 

# monthly active membership eligible action count by action type 
mam_actions <- MAM_action_feature(mel)

# cause space 
cause_space <- cause_space_feature(users, campaign, campaign_info)

# months before last active
last_action <- last_action_feature(mel)
```

# Create a dataset with desired features 

```{r}
master <- 
  reg_status %>% 
  left_join(gender, by = "northstar_id") %>% 
  left_join(age, by = "northstar_id") %>%
  left_join(ses, by = "northstar_id") %>% 
  left_join(state_politics, by = "northstar_id") %>%
  left_join(email_count, by = "northstar_id") %>% 
  left_join(sms_count, by = "northstar_id") %>%
  left_join(browser_size, by = "northstar_id") %>% 
  left_join(total_action, by = "northstar_id") %>%
  left_join(rb_status, by = "northstar_id") %>%
  left_join(time_of_day, by = "northstar_id") %>%
  left_join(mam_actions, by = "northstar_id") %>%
  left_join(cause_space, by = "northstar_id") %>% 
  left_join(last_action, by = "northstar_id")

# MUST replace NA's in numeric columns with either 0s or means for model to run  
vector_mean <- "age" # age or any other appropriate column that should be imputed with mean
vector_zeros <- # vector of column names 
  colnames(master %>% 
             select_if(is.numeric) %>% 
             select(-age)) 
master <- replace_na_numeric(master, vector_mean, vector_zeros) 

# MUST replace NAs in non-numeric columns with "unknown" for model to run
master <- replace_na_else(master) 
```


## Descriptive Statistics 

```{r}
# distribution of voter_reg statuses 
voter_reg_freq <- 
  master %>% 
  group_by(voter_reg_status) %>% 
  tally

ggplot(voter_reg_freq, aes(x = reorder(voter_reg_status, n), y = n, fill = voter_reg_status)) + 
  geom_bar(stat = "identity") + 
  geom_text(aes(label = n), vjust = -0.5, size = 3) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Breakdown of DoSomething voter registration status",
       x = "voter registration status", 
       y = "frequency")
```

```{r}
member_gender <- 
  master %>% 
  filter(!is.na(gender)) %>% 
  group_by(gender) %>% 
  tally %>% 
  mutate(perc = round((n / sum(n)), 2))

ggplot(member_gender, aes(x = "", y = n, fill = gender)) + 
  geom_bar(stat = "identity", width = 1, position = position_fill())+ 
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.title = element_blank(),
        axis.line = element_blank(),
        panel.grid = element_blank()) + 
  scale_y_continuous(breaks = NULL) +
  geom_text(aes(x = 1, y = cumsum(perc) - perc/2, label = perc)) +
  labs(title = "Gender breakdown of DoSomething members")
  
```

```{r}
voter_reg_gender <- 
  master %>% 
  filter(!is.na(gender) & voter_reg_status != "ineligible") %>%
  group_by(voter_reg_status, gender) %>% 
  tally
voter_reg_gender$voter_reg_status <- factor(voter_reg_gender$voter_reg_status,
                                            levels = c("any_interaction", "registration_started", "registration_complete"),
                                            labels = c("any_interaction", "registration_started", "registration_complete"))
  
# pie chart for gender breakdown 
ggplot(voter_reg_gender, aes(x = "", y = n, fill = gender)) + 
  geom_bar(stat = "identity", width = 1, position = position_fill()) + 
  coord_polar(theta = "y") + 
  facet_wrap(~ voter_reg_status, ncol = 3) +
  theme(axis.ticks = element_blank(),
        axis.title = element_blank(),
        axis.line = element_blank(),
        panel.grid = element_blank()) +
  scale_y_continuous(breaks = NULL) +
  labs(title = "Gender breakdown by registration status")
  

# ses breakdown, behavioral stats (total action, reportback status), the NA's of categorical variables, what else?!? 
```


## Identify & eliminate near zero-variance variables

(zero-variance variables are variables that have only a handful of unique values that occur with very low frequency - may need to be eliminated prior to modeling for fit to be stable)

```{r}
df_list <- remove_nzv(master) 

master <- df_list[[2]] # keeps only the dataframe
```

## Identify & eliminate highly correlated predictors

```{r}
df_list2 <- remove_highcor(master)

master <- df_list2[[3]] # keeps only the dataframe
```

# Find & remove linear dependencies 

```{r}
numeric_only <- master %>% select_if(is.numeric)
comboinfo <- findLinearCombos(numeric_only) # nothing to remove
```

## Finalize master dataset with desired ordered factor levels 
(Creates a sample of no_interaction category data from NA's)

```{r}
master <- master_final(master)
```

## Remove datasets no longer needed to free up RAM 

```{r}
rm(age, browser_size, email_count, gender, rb_status, reg_status,
   ses, sms_count, state_politics, time_of_day, total_action, df_list, df_list2)

rm(campaign, email, mel, phoenix,
   sms, turbo, users)

rm(mam_actions, campaign_info, campaign_with_id, cause_space, comboinfo, output)

rm(numeric_only, voter_reg_freq, voter_reg_gender)
```


# Split into training & testing

### Divide into training and testing 

```{r}
in_train <- createDataPartition(y = master$voter_reg_status, p = 3 / 4, list = FALSE)
training <- master[in_train, ]
testing  <- master[-in_train, ]
```

The dataset is unbalanced because there is a significantly greater number of "interaction" than "registration complete" & "uncertain" values, which would bias the model into favoring predictions for those who have "interacted" with any voter registration CTA. I will use the caret upsample function to increase the other two values, with replacement. 

(The up sampling should be done to the training set ONLY. The testing set should maintain the unbalanced classes.)  

```{r}
up_train <- upSample(x = select(training, -c(voter_reg_status, northstar_id)),
                     y = training$voter_reg_status,
                     yname = "voter_reg_status")
```

Centering and scaling data ensures that some features do not dominate the algorithm AND and the features are unit-independent. 

```{r}
preprocvalues <- preProcess(up_train, method = c("center", "scale")) # estimates required parameters to be centered & scaled 
train_transformed <- predict(preprocvalues, up_train) # this function actually centers and scales based on the parameters defined above 
test_transformed <- predict(preprocvalues, testing)
```

randomly sample b/c of memory issues....

```{r}
by_grp <- train_transformed %>% group_by(voter_reg_status)
train_transformed_sample <- sample_n(by_grp, 10000)

rm(master, by_grp, in_train, testing, train_transformed, training, up_train)
```


Using the caret package to set a 10-fold cross-validation for models with tuning parameters. 

```{r}
ctrl <- trainControl(method = "cv",
                     number = 10,
                     summaryFunction = multiClassSummary, 
                     classProbs = TRUE)
```

# Model Building and Assessment 

## Random Forest 

```{r}
rf_fit <- train(voter_reg_status ~ ., 
                data = train_transformed_sample, # likely to improve with bigger training sample
                method = "rf",
                trControl = ctrl,
                verbose = FALSE)
rf_fit$bestTune
rf_model <- randomForest(voter_reg_status ~ .,
                         data = train_transformed_sample,
                         mtry = 10,
                         importance = TRUE)
rf_model$importance
rf_pred <- predict(rf_model, newdata = test_transformed, type = "class")
rf_results <- mod_perf_function("random_forest", rf_pred, test_transformed)
```


## Ordinal Logistic

```{r}
# rank-deficient: ses_status
# glm.fit (fitted probabilities numerically 0 or 1 occurred): total_action, afternoon_activities

train_ord <- 
  train_transformed_sample %>% 
  select(-c(ses_status, total_action, afternoon_activities)) # remove variables that are causing above errors 

ord_logit_model <- MASS::polr(voter_reg_status ~ .,
                  data = train_ord) 
ord_logit_model
ord_logit_pred <- predict(ord_logit_model, newdata = test_transformed, type = "class")
ord_logit_results <- mod_perf_function("ordinal_logit", ord_logit_pred, test_transformed)
```


## Neural Network 

```{r}
nnet_fit <- train(voter_reg_status ~ ., 
                data = train_transformed_sample,
                method = "nnet",
                trControl = ctrl,
                verbose = FALSE)
nnet_fit$bestTune

nnet_model <- nnet(voter_reg_status ~ .,
                  data = train_transformed_sample,
                  size = 5, # from nnet_fit$bestTune
                  decay = 0.1) # from nnet_fit$bestTune
nnet_pred <- predict(nnet_model, newdata = test_transformed, type = "class")
nnet_pred <- factor(nnet_pred,
                    levels = c("no_interaction", "any_interaction", 
                               "registration_started", "registration_complete"),
                    labels = c("no_interaction", "any_interaction", 
                               "registration_started", "registration_complete"))
nnet_results <- mod_perf_function("nnet", nnet_pred, test_transformed)
```

## xgboost (linear)

```{r}
xgboost_fit <- train(voter_reg_status ~ .,
                     data = train_transformed_sample,
                     method = "xgbLinear",
                     trControl = ctrl,
                     verbose = FALSE)
xgboost_fit$bestTune

# transform training data to work with xgboost 
train_xgboost <- xgboost_data_transform_function(train_transformed_sample)

# transform testing data to work with xgboost
test_xgboost <- xgboost_data_transform_function(test_transformed)

xgboost_model <- xgb.train(data = train_xgboost,
                           nrounds = 150,
                           lambda = 0, 
                           alpha = 0.0001,
                           eta = 0.3,
                           objective = "multi:softmax",
                           num_class = 4)
xgboost_pred <- predict(xgboost_model, 
                        newdata = test_xgboost,
                        reshape = TRUE) 

xgboost_pred <- factor(xgboost_pred,
                       levels = c(0:3),
                       labels = c("no_interaction", "any_interaction", "registration_started", "registration_complete"))

xgboost_results <- mod_perf_function("xgoobst", xgboost_pred, test_transformed)
```

## xgboost (tree)

```{r}
xgboost_tree_fit <- train(voter_reg_status ~ .,
                     data = train_transformed_sample,
                     method = "xgbTree",
                     trControl = ctrl,
                     verbose = FALSE)
xgboost_tree_fit$bestTune

xgboost_tree_model <- xgb.train(data = train_xgboost,
                                nrounds = 150,
                                max_depth = 3, 
                                eta = 0.4,
                                gamma = 0,
                                colsample_bytree = 0.8,
                                min_child_weight = 1, 
                                subsample = 0.75,
                                objective = "multi:softmax",
                                num_class = 4)
xgboost_tree_pred <- predict(xgboost_tree_model, 
                             newdata = test_xgboost,
                             reshape = TRUE)

xgboost_tree_pred <- factor(xgboost_tree_pred,
                       levels = c(0:3),
                       labels = c("no_interaction", "any_interaction", "registration_started", "registration_complete"))

xgboost_tree_results <- mod_perf_function("xgboost_tree", xgboost_tree_pred, test_transformed)
```


## gbm 

```{r}
gbm_fit <- train(voter_reg_status ~ ., 
                 data = train_transformed_sample,
                 method = "gbm",
                 trControl = ctrl, 
                 verbose = FALSE)
gbm_fit$bestTune # includes the best parameters 
gbm_model <- gbm(voter_reg_status ~ .,
                 distribution = "multinomial",
                 data = train_transformed_sample,
                 n.trees = 150, # parameter determined by 10-fold cross validation
                 interaction.depth = 3, # parameter determined by 10-fold cross validation
                 shrinkage = 0.1, # parameter determined by 10-fold cross validation
                 n.minobsinnode = 10) # parameter determined by 10-fold cross validation
gbm_pred <- predict(gbm_model, 
                    newdata = test_transformed,
                    n.trees = 150,
                    type = "response")
gbm_pred <- gbm_transform_function(gbm_pred) # transform predictions into a dataset that can be compared to the test dataset

gbm_results <- mod_perf_function("gbm", gbm_pred, test_transformed)
```


## Comparing performance of all models 

```{r}
samp_results <- mod_perf_function("model1", xgboost_tree_pred, test_transformed)

```















