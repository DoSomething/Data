---
title: "voter_reg_modeling"
output: html_document
---

############ Initial Predictions ###################

# 1. Sources & Libraries 

```{r, quietly=TRUE}
source('config/init.R')
source('config/pgConnect.R')
source('config/data_source.R') # data sources
source('config/voter_regfunctions.R') # all functions are here
pg <- pgConnect()

packages.used <- c("lubridate", "reshape2", "caret", "ggplot2", "MLmetrics", "e1071",
                   "nnet", "xgboost", "gbm", "randomForest", "zoo", "rlang", "dplyr")

#check packages that need to be installed
packages.needed <- setdiff(packages.used, intersect(installed.packages()[,1], packages.used))

#install additional packages 
if (length(packages.needed) > 0) {
  install.packages(packages.needed, dependencies = TRUE)
}

library(lubridate)
library(reshape2)
library(caret)
library(ggplot2)
library(MLmetrics)
library(e1071)
library(nnet)
library(xgboost)
library(gbm)
library(randomForest)
library(zoo)
library(rlang)
library(dplyr)

options(mc.cores = parallel::detectCores())
```

# 2. Load datasets (Depending on when "users" dataset last refreshed)

```{r}
data_list <- load_data(days_refreshed = 0)
list2env(data_list, .GlobalEnv)
```

# 3. Data wrangling 

## Extracting predicted feature (voter registration status)

```{r}
# a part of this code will soon become obsolete as Jen is leading the sprint to combine for all columns 
reg_status <- predicted_feature(users, turbo, campaign, phoenix, email)
```


## Selecting demographic features 

```{r}
# gender feature 
gender <- gender_feature(users)

# age feature 
age <- age_feature(users)

# SES feature 
ses <- ses_feature(users)

# state and whether it turned blue/red in 2016 election 
state_politics <- dem_rep_feature(users)
```

## Selecting behavioral features 

```{r}
# returns variables of email clicked, converted, opened, unsubscribed & total number of email actions per northstar_id
email_count <- email_feature(email)

# returns columns of SMS action by type & total number of sms actions per northstar_id 
sms_count <- sms_total_feature(sms)

# returns columns of browser size by northstar_id 
browser_size <- browser_size_feature(phoenix)

# returns number of total actions per northstar_id 
total_action <- total_actions_feature(phoenix, mel)

# how many reportbacks a user has done 
rb_count <- rb_feature(campaign, users)

# returns time of day when most active (morning, afternoon, evening, night)
time_mostactive <- time_mostactive_feature(phoenix, mel) 

# monthly active membership eligible action count by action type 
mam_actions <- MAM_action_feature(mel)

# cause space 
cause_space <- cause_space_feature(users, campaign, campaign_info)

# days since last active
last_action <- last_action_feature(mel)
```

## Create a dataset with desired features 

```{r}
master <- plyr::join_all(list(reg_status,
                              gender,
                              age,
                              ses,
                              state_politics,
                              email_count,
                              sms_count,
                              browser_size,
                              total_action,
                              rb_count,
                              time_mostactive,
                              mam_actions,
                              cause_space,
                              last_action),
                         by = "northstar_id",
                         type = "left")
```


## Replace NAs 

```{r}
# replace NA's in numeric columns with either 0s or means
vector_mean <- "age" # age or any other appropriate column that should be imputed with mean
vector_zeros <- # vector of column names 
  colnames(master %>% 
             select_if(is.numeric) %>% 
             select(-age)) 
master <- replace_na_numeric(master, vector_mean, vector_zeros) 

# replace NAs in non-numeric columns (except voter_reg_status) with "unknown" 
master <- replace_na_else(master) 
```


## Identify & eliminate near zero-variance variables

(zero-variance variables are variables that have only a handful of unique values that occur with very low frequency - may need to be eliminated prior to modeling for fit to be stable)

```{r}
df_list <- remove_nzv(master) 

master <- df_list[[2]] # keeps only the dataframe
```

## Identify & eliminate highly correlated predictors

```{r}
df_list2 <- remove_highcor(master)

master <- df_list2[[3]] # keeps only the dataframe
```

## Find & remove linear dependencies 

```{r}
numeric_only <- master %>% select_if(is.numeric)
comboinfo <- findLinearCombos(numeric_only) # nothing to remove
```

## Save for quasar table 

```{r}
# all user records + untransformed predictors & predicted 
saveRDS(master, "join_quasar.rds")
```

## Centering and scaling data 

Ensures that some features do not dominate the algorithm and the features are unit-independent. 

```{r}
preprocvalues <- preProcess(master, method = c("center", "scale")) # estimates required parameters to be centered & scaled 
master <- predict(preprocvalues, master) # this function actually centers and scales based on the parameters defined above 
```

## Save data for prediction 
(Only includes voter_reg_status = NA rows whose likelihood score will be predicted)

```{r}
data_ready_predict <- 
  master %>% 
  filter(is.na(voter_reg_status))

# save data for final prediction
saveRDS(data_ready_predict, "data_ready_predict.rds")
```

## Finalize modeling dataset with desired ordered factor levels 
(Creates a sample of no_interaction category data from NA's)

```{r}
master <- master_final(master)
```

## Remove datasets no longer needed to free up RAM 

```{r}
rm(age, browser_size, email_count, gender, rb_status, reg_status,
   ses, sms_count, state_politics, time_of_day, total_action, df_list, df_list2, 
   campaign, email, mel, phoenix, sms, turbo, users, mam_actions, campaign_info, 
   campaign_with_id, cause_space, comboinfo, output) 
```

# 4. Modeling 

## Split into training & testing

```{r}
in_train <- createDataPartition(y = master$voter_reg_status, p = 3 / 4, list = FALSE)
training <- master[in_train, ]
testing  <- master[-in_train, ]
```

## Balance the dataset

The dataset is unbalanced because there is a significantly greater number of "any_interaction" & "no_interaction" classes than "registration complete" & "registration_started" classes, which would bias the model into favoring predictions for the higher frequency classes. 

I am going to group by and randomly sample from all classes to create a balanced dataset. 

```{r}
training <- balance_data(training, 50000)
```

```{r}
saveRDS(testing, "testing.rds")
saveRDS(training, "training.rds")

rm(in_train, master, testing, training)
```

Using the caret package to set a 10-fold cross-validation for models with tuning parameters. 

```{r}
ctrl <- trainControl(method = "cv",
                     number = 10,
                     summaryFunction = multiClassSummary, 
                     classProbs = TRUE)
```

remove northstar_id (character variable causes issues for some models)

```{r}
training <- 
  training %>% 
  select(-northstar_id)

set.seed(12345)
```


## Model development and assessment

### Random Forest 

```{r}
rf_fit <- train(voter_reg_status ~ .
                data = training, 
                method = "rf",
                trControl = ctrl,
                verbose = FALSE)
rf_fit$bestTune
rf_model <- randomForest(voter_reg_status ~ .,
                         data = training,
                         mtry = 7,
                         importance = TRUE)
varImpPlot(rf_model)
rf_model$importance
rf_pred <- predict(rf_model, newdata = testing, type = "class")
rf_results <- mod_perf_function("random_forest", rf_pred, testing)
```


### Ordinal Logistic

```{r}
# rank-deficient: ses_status
# glm.fit (fitted probabilities numerically 0 or 1 occurred): site_login

train_ord <- 
  training %>% 
  select(-c(ses_status, site_login)) # remove variables that are causing above errors 

ord_logit_model <- MASS::polr(voter_reg_status ~ .,
                              data = train_ord) 
ord_logit_model
ord_logit_pred <- predict(ord_logit_model, newdata = testing, type = "class")
ord_logit_results <- mod_perf_function("ordinal_logit", ord_logit_pred, testing)
```


### Neural Network 

```{r}
nnet_fit <- train(voter_reg_status ~ ., 
                data = training,
                method = "nnet",
                trControl = ctrl,
                verbose = FALSE)
nnet_fit$bestTune

nnet_model <- nnet(voter_reg_status ~ .,
                  data = training,
                  size = 5, 
                  decay = .0001)
nnet_pred <- predict(nnet_model, newdata = testing, type = "class")
nnet_pred <- factor(nnet_pred,
                    levels = c("no_interaction", "any_interaction", 
                               "registration_started", "registration_complete"),
                    labels = c("no_interaction", "any_interaction", 
                               "registration_started", "registration_complete"))
nnet_results <- mod_perf_function("nnet", nnet_pred, testing)
```

### xgboost (linear)

```{r}
xgboost_fit <- train(voter_reg_status ~ .,
                     data = training,
                     method = "xgbLinear",
                     trControl = ctrl,
                     verbose = FALSE)
xgboost_fit$bestTune

# transform training data to work with xgboost 
train_xgboost <- xgboost_data_transform_function(training)

# transform testing data to work with xgboost
test_xgboost <- xgboost_data_transform_function(testing)

xgboost_model <- xgb.train(data = train_xgboost,
                           nrounds = 150,
                           lambda = 0.0001, 
                           alpha = 0,
                           eta = 0.3,
                           objective = "multi:softmax",
                           num_class = 4)
xgboost_pred <- predict(xgboost_model, 
                        newdata = test_xgboost,
                        reshape = TRUE) 

xgboost_pred <- factor(xgboost_pred,
                       levels = c(0:3),
                       labels = c("no_interaction", "any_interaction", "registration_started", "registration_complete"))

xgboost_results <- mod_perf_function("xgoobst", xgboost_pred, testing)
```


### xgboost (tree)

```{r}
xgboost_tree_fit <- train(voter_reg_status ~ .,
                     data = training,
                     method = "xgbTree",
                     trControl = ctrl,
                     verbose = FALSE)
xgboost_tree_fit$bestTune

xgboost_tree_model <- xgb.train(data = train_xgboost,
                                nrounds = 150,
                                max_depth = 3, 
                                eta = 0.4,
                                gamma = 0,
                                colsample_bytree = 0.8,
                                min_child_weight = 1, 
                                subsample = 0.75,
                                objective = "multi:softmax",
                                num_class = 4)
xgboost_tree_pred <- predict(xgboost_tree_model, 
                             newdata = test_xgboost,
                             reshape = TRUE)

xgboost_tree_pred <- factor(xgboost_tree_pred,
                       levels = c(0:3),
                       labels = c("no_interaction", "any_interaction", "registration_started", "registration_complete"))

xgboost_tree_results <- mod_perf_function("xgboost_tree", xgboost_tree_pred, testing)
```


### gbm 

```{r}
gbm_fit <- train(voter_reg_status ~ ., 
                 data = training,
                 method = "gbm",
                 trControl = ctrl, 
                 verbose = FALSE)
gbm_fit$bestTune # includes the best parameters 
gbm_model <- gbm(voter_reg_status ~ .,
                 distribution = "multinomial",
                 data = samp,
                 n.trees = 150, # parameter determined by 10-fold cross validation
                 interaction.depth = 3, # parameter determined by 10-fold cross validation
                 shrinkage = 0.1, # parameter determined by 10-fold cross validation
                 n.minobsinnode = 10) # parameter determined by 10-fold cross validation
gbm_pred <- predict(gbm_model, 
                    newdata = testing,
                    n.trees = 150,
                    type = "response")
gbm_pred <- gbm_transform_function(gbm_pred) # transform predictions into a dataset that can be compared to the test dataset

gbm_results <- mod_perf_function("gbm", gbm_pred, testing)
```


### Comparing performance of models 

```{r}
model_performance <- model_comparison(rf_results, ord_logit_results, nnet_results, 
                                      xgboost_results, xgboost_tree_results, gbm_results)

ranked_models <- model_select(model_performance, "AUC: registration_complete")

# best model is saved 
saveRDS(ord_logit_model, "models/best_model_ordlogit.rds")
```

# Select model and save model for future predictions 

# Making final prediction & creating a table to load into Quasar

```{r}
# use data not trained and tested and which northstar_id took no action (voter_reg_status = NA)
data_ready_predict <- readRDS("future_predictions/data_ready_predict.rds") 

# create a table that has northstar_id, voter_reg_status, untransformed predictors, and predictions 
voter_reg_prediction <- voter_reg_predict(data_ready_predict, "future_predictions/join_quasar.rds")
```


# Write it into Quasar 

I have the final prediction, load it into Quasar

```{r}
channel <- pgConnect()

if(dbExistsTable(channel, c("public", "voter_reg_predictions"))) {
  dbRemoveTable(channel, c("public", "voter_reg_predictions"))}

dbWriteTable(channel, c("public", "voter_reg_predictions"), voter_reg_prediction, row.names=F)

```


#################### Add predictions for new users #######################

## Extract new members since last refreshed 

```{r}
data_list_new <- new_users_only()
list2env(data_list_new, .GlobalEnv)
```

## Create dataset with predicted and predictors 

```{r}
master <- create_master()
```

## Center and scale data

```{r}
preprocvalues <- preProcess(master, method = c("center", "scale")) 
master <- predict(preprocvalues, master) 
```

## Extract data with no voter_reg_status for predictions 

```{r}
predictions_only  <- 
  master %>% 
  filter(is.na(voter_reg_status))
```

## Make predictions 

```{r}
voter_reg_prediction <- voter_reg_predict(predictions_only, "join_quasar.rds")
```

## Add it to Quasar table 

```{r}
channel <- pgConnect()
dbWriteTable(channel, 
             c("public", "voter_reg_predictions"), 
             voter_reg_prediction, 
             append=T, row.names=F)
```

















